# Metodología {.unlisted .unnumbered}

## Indicadores simples {-}

$$Indicador_{j}=\frac{\sum_{i=1}^{k}Z_{ij}\sqrt{\lambda_{i}}}{\sum_{i=1}^{k}\sqrt{\lambda_{i}}}; \text{i = 1, 2, ..., n componentes}$$

Siendo:  

- $Z_{ij}$ la puntuación del componente r-ésimo para la unidad de observación j-ésima.




##  Análisis de Componentes Principales `PCA`  {-} 

El análisis de Componentes Principales  `PCA` se refiere a un método estadístico utilizado para reducir la complejidad de un conjunto de datos. El objetivo es transformar las variables originales en un conjunto más pequeño y manejable de nuevas variables llamadas "componentes principales".   

Este método se genera apartir del cálculo de una matriz de covarianzas o de correlaciones entre las variables originales, y la posterior obtención de los vectores y valores propios de esta matriz. Los vectores propios son las direcciones en las que los datos tienen la mayor variabilidad, y los valores propios indican la cantidad de variabilidad que se explica por cada componente principal.  Donde los valores propios obtenidos se ordenan de mayor a menor, de tal manera que el primer componente principal explica la mayor parte de la variabilidad en los datos, y cada componente subsiguiente explica una proporción menor. Los componentes principales pueden ser utilizados para describir los datos y para la construcción de modelos estadísticos más sencillos.      

En resumen, los componentes principales son un método estadístico para reducir la complejidad de un conjunto de datos, transformando las variables originales en un conjunto más pequeño de nuevas variables llamadas componentes principales, que explican la mayor parte de la variabilidad en los datos o bien dicho el mayor porcentaje de información de los datos originales.      
 
El método de componentes principales (PCA) pueden tener varias ventajas y desventajas que se deben considerar al utilizarlo:   

**Ventajas:**

- Reducción de la dimensionalidad: PCA permite reducir el número de variables que se utilizan para describir un conjunto de datos, lo que facilita la visualización y el análisis de los datos.   

- Identificación de patrones: Al identificar los componentes principales, PCA puede ayudar a encontrar patrones o relaciones entre las variables que no eran evidentes en los datos originales.  

- Eliminación de variables redundantes: PCA puede eliminar variables que estén altamente correlacionadas, lo que puede mejorar la eficiencia computacional y la interpretación de los resultados.   

- Normalización de los datos: PCA normaliza los datos, lo que puede ayudar a eliminar problemas de escala o unidades diferentes entre las variables originales.  

**Desventajas:**    

- Interpretación: La interpretación de los componentes principales puede ser difícil, especialmente si no se tiene un conocimiento profundo del conjunto de datos.

- Pérdida de información: PCA puede eliminar información útil si se eliminan componentes que no tienen una gran variabilidad, pero que podrían ser importantes en la descripción de los datos.

- Sensibilidad a los datos atípicos: PCA puede verse afectado por los datos atípicos, lo que puede resultar en componentes que no reflejan bien la variabilidad en los datos.

-  Requisitos de computación: PCA puede requerir muchos recursos de computación, especialmente para conjuntos de datos grandes.


### Descomposición de componentes principales {-}

En el análisis de componentes principales, se puede descomponer la matriz de varianzas y covarianzas de dos maneras diferentes:

Descomposición en valores propios (eigendecomposition): Esta descomposición se utiliza cuando la matriz de varianzas y covarianzas es simétrica y definida positiva. En este caso, la matriz se descompone en una matriz de autovectores y una matriz diagonal de valores propios. Los autovectores forman una base ortogonal que describe las direcciones principales de la varianza en los datos, mientras que los valores propios indican la importancia relativa de cada dirección.

Descomposición en valores singulares (singular value decomposition, SVD): Esta descomposición se utiliza cuando la matriz de varianzas y covarianzas no es simétrica o no es definida positiva. En este caso, la matriz se descompone en tres matrices: una matriz de autovectores izquierdos, una matriz de autovectores derechos y una matriz diagonal de valores singulares. Los autovectores izquierdos y derechos forman una base ortogonal que describe las direcciones principales de la varianza en los datos, mientras que los valores singulares indican la importancia relativa de cada dirección.

En ambas descomposiciones, se pueden seleccionar las componentes principales más importantes para reducir la dimensionalidad de los datos y simplificar su análisis.   


### Identificación de datos atípicos {-}   

Los datos atípicos (también conocidos como valores extremos o outliers) pueden tener un impacto significativo en los resultados del método de componentes principales (`PCA`). Si los datos atípicos no se manejan adecuadamente, pueden llevar a resultados sesgados o incorrectos.    

Existen algunas estrategias para manejar los datos atípicos dentro del análisis de  PCA:

- Identificación de los datos atípicos: Es importante identificar los datos atípicos antes de realizar el análisis de PCA. Esto se puede hacer mediante la visualización de los datos o mediante pruebas estadísticas.   

- Exclusión de los datos atípicos: Una estrategia común para manejar los datos atípicos es excluirlos del análisis. Sin embargo, esto debe hacerse con cuidado, ya que la exclusión de los datos atípicos puede resultar en una pérdida de información importante.   

- Transformación de los datos: Otra estrategia es transformar los datos antes de realizar el análisis de PCA. Las transformaciones comunes incluyen la transformación logarítmica o la normalización de los datos. Estas transformaciones pueden reducir la influencia de los datos atípicos y mejorar los resultados de PCA.   

- Uso de métodos robustos: Existen métodos robustos que pueden manejar los datos atípicos de manera efectiva en PCA, como el método de componentes principales robustos (RPCA, por sus siglas en inglés). Estos métodos son menos sensibles a los datos atípicos y pueden proporcionar resultados más precisos.  

- En resumen, los datos atípicos pueden tener un impacto significativo en los resultados de PCA. Es importante identificarlos y considerar cuidadosamente cómo manejarlos antes de realizar el análisis de PCA.  

En resumen, el método de componentes principales puede ser una herramienta valiosa para reducir la complejidad de los datos y encontrar patrones en ellos, pero es importante tener en cuenta sus limitaciones y considerar cuidadosamente cuándo y cómo utilizarlo.     



### Método de Componentes Principales Robustos `RPCA`  {-}

El método de componentes principales robustos (RPCA, por sus siglas en inglés) se utiliza cuando se sospecha que los datos contienen valores atípicos o errores que pueden afectar los resultados del análisis de componentes principales (PCA).

RPCA es una versión modificada del método de componentes principales tradicional que es menos sensible a los valores atípicos y a los errores en los datos. En lugar de minimizar la varianza total de los datos como en PCA, RPCA minimiza la varianza total de los datos después de excluir los valores atípicos o errores.

RPCA puede ser útil en una variedad de situaciones, como en el análisis de datos biomédicos, análisis financiero, análisis de datos climáticos, entre otros.   
 
En general, se recomienda utilizar RPCA cuando:

- Los datos pueden contener valores atípicos o errores.

- Los datos tienen una distribución no normal.

- Las variables tienen diferentes escalas.

- El tamaño de la muestra es pequeño en comparación con la dimensión de los datos.

- Se necesita una mayor precisión en los resultados del análisis.

**Observaciones del método RPCA**

Es importante tener en cuenta que RPCA no siempre es necesario y puede ser computacionalmente más costoso que el método de componentes principales tradicional. Por lo tanto, es importante evaluar cuidadosamente si RPCA es apropiado para los datos y los objetivos de análisis antes de utilizarlo.     

Aunque el método de componentes principales robustos (RPCA) es útil para manejar valores atípicos y errores en los datos, también presenta algunas desventajas que deben tenerse en cuenta:   

- Mayor complejidad computacional: El método RPCA puede ser más costoso computacionalmente que el método de componentes principales tradicional debido a la necesidad de excluir los valores atípicos o errores en los datos.  

- Pérdida de información: Al excluir los valores atípicos o errores en los datos, puede perderse información importante, lo que puede afectar la precisión de los resultados del análisis.  

- Dependencia del modelo: El método RPCA se basa en un modelo específico de distribución de datos, por lo que si los datos no se ajustan a este modelo, los resultados pueden ser incorrectos o inexactos.  

- Selección de parámetros: El método RPCA requiere la selección de parámetros para determinar qué valores se consideran atípicos o errores, lo que puede ser subjetivo y afectar los resultados del análisis.  

En general, es importante evaluar cuidadosamente si el método RPCA es apropiado para los datos y los objetivos de análisis antes de utilizarlo, y considerar tanto sus ventajas como sus desventajas.  


## Regresión de Componentes Principales (`PCR`) {-}

El modelo de PCR es un método utilizado para reducir la dimensionalidad de los datos y evitar problemas de multicolinealidad en un modelo de regresión lineal múltiple. Este modelo se basa en la descomposición en valores singulares (SVD) de la matriz de datos originales, lo que permite identificar las componentes principales que explican la mayor parte de la varianza en los datos.

A partir de las componentes principales seleccionadas, se construye un modelo de regresión lineal múltiple utilizando estas componentes como variables predictoras. El número de componentes principales seleccionadas se determina de acuerdo con un criterio predefinido, como la cantidad de varianza explicada o un límite para el número de componentes.

El modelo de PCR puede ser una alternativa útil a la selección tradicional de variables predictoras en un modelo de regresión lineal, ya que permite reducir la dimensionalidad de los datos y evitar problemas de multicolinealidad. Sin embargo, es importante tener en cuenta que el modelo de PCR también tiene algunas limitaciones, como la interpretación de los coeficientes de regresión y la posible pérdida de información importante en los datos. Por lo tanto, se recomienda utilizar el modelo de PCR en combinación con otras técnicas de análisis de datos para obtener una comprensión completa del problema.


**Singular value descomposición**  

En el modelo de Regresión de Componentes Principales (PCR, por sus siglas en inglés), se utiliza la descomposición en valores singulares (SVD) para encontrar las componentes principales de los datos y reducir su dimensionalidad.

La idea detrás del PCR es utilizar las componentes principales de los datos (obtenidas a través de la SVD) como variables predictoras en un modelo de regresión lineal múltiple. De esta manera, se puede reducir el número de variables predictoras y evitar problemas de multicolinealidad, lo que puede mejorar la precisión del modelo y hacerlo más interpretable.

En resumen, para implementar el modelo de PCR se utilizan los resultados de la descomposición en valores singulares (SVD) para seleccionar las componentes principales de los datos y construir un modelo de regresión lineal múltiple. Por lo tanto, no se utiliza la descomposición en eigenvectores y eigenvalores.  


## Método de distancias ponderadas al cuadrado $DP_2$ {-}

El método de distancias ponderadas al cuadrado (DP2) de José Bernardo Peña Trapero es una técnica estadística utilizada para medir la similitud entre diferentes objetos o casos. Este método se basa en una medida de distancia que se calcula sumando las diferencias al cuadrado ponderadas entre las características de dos objetos y tomando la raíz cuadrada del resultado. La ponderación de las características permite que las más importantes tengan un mayor impacto en el resultado final mientras que las menos importantes tienen un menor impacto.

El método DP2 de Peña Trapero se utiliza en una variedad de aplicaciones, como la clasificación de imágenes y la evaluación de la similitud entre diferentes textos. Una de las ventajas del método DP2 es que puede manejar datos de diferentes tipos y escalas, lo que lo hace útil en situaciones donde los datos son heterogéneos.

Sin embargo, también hay algunas desventajas en el uso del método DP2, como la selección subjetiva de la función de ponderación de características y el costo computacional en grandes conjuntos de datos. Por lo tanto, se deben evaluar cuidadosamente los datos y los objetivos de análisis antes de aplicar este método.

En resumen, el método DP2 de Peña Trapero es una técnica útil para medir la similitud entre diferentes objetos o casos, pero se debe aplicar con cuidado y evaluar cuidadosamente en función de los datos y objetivos de análisis.

