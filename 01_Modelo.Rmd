# Metodología {.unlisted .unnumbered}

<a href="img/Metodología ICE.png" data-lightbox="image-1" data-title="metodologia">
```{r, echo = FALSE, fig.width=12, fig.height=12, out.width='100%'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/Metodología ICE.png"))
```
</a>


## Primera etapa: construcción de indicadores {-} 

De acuerdo con los 13 subsistemas de equipamiento, se tomaron en consideración 71 variables que agruparon a los sectores propuesto en el Sistema Normativo de Equipamiento (SEDESOL, 2012). Dada las limitaciones en la disponibilidad de datos e información oficial disponible para la conformación de las variables, estas se construyeron como porcentaje de los equipamientos disponibles en las localidades respecto al total del municipio y sector correspondiente:   

$$w_{hjk} = \sum_{h=1}^{N_{h}} \sum_{j=1}^{n_{h}}\frac{S_{hjk}}{S_{hk}}; \text{donde k: 1,2,...,m} $$
donde: 

- $w_{hjk}$: Razón de la característica de interés en la $j-ésima$ localidad del $h-ésimo$ municipio,    
- $n_{h}$: Número de localidades en el $h-ésimo$ municipio,    
- $N_{h}$: Total de localidades en el $h-ésimo$ municipio,   
- $S_{hk}$: Total de la característica de interés del $h-ésimo$ municipio,   
- $S_{hk}$: Total de la característica de interés en la localidad $j-ésima$ del $h-ésimo$ municipio.   


## Segunda etapa: Análisis de Componentes Principales (PCA) {-}   

El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es un método estadístico utilizado para reducir la complejidad de un conjunto de datos. El objetivo es transformar las variables originales en un conjunto más pequeño y manejable de nuevas variables llamadas "componentes principales".   

Este método se basa en el cálculo de una matriz de covarianzas o correlaciones entre las variables originales, y en la posterior obtención de los vectores y valores propios de esta matriz. Los vectores propios representan las direcciones en las que los datos tienen la mayor variabilidad, y los valores propios indican la cantidad de variabilidad explicada por cada componente principal. Los valores propios se ordenan de mayor a menor, de manera que el primer componente principal explica la mayor parte de la variabilidad en los datos, y cada componente subsiguiente explica una proporción menor. Los componentes principales se utilizan para describir los datos y construir modelos estadísticos más sencillos.  

En resumen, el PCA es un método estadístico que reduce la complejidad de un conjunto de datos al transformar las variables originales en un conjunto más pequeño de nuevas variables que explican la mayor parte de la variabilidad de los datos.   

### Ventajas y Desventajas del PCA {-}  

**Ventajas:**  

- **Reducción de la dimensionalidad**: PCA permite reducir el número de variables utilizadas para describir un conjunto de datos, facilitando su visualización y análisis.   
- **Identificación de patrones**: Al identificar los componentes principales, el PCA puede ayudar a encontrar patrones o relaciones entre las variables que no eran evidentes en los datos originales.   
- **Eliminación de variables redundantes**: PCA puede eliminar variables altamente correlacionadas, mejorando la eficiencia computacional y la interpretación de los resultados.   
- **Normalización de los datos**: PCA normaliza los datos, eliminando problemas de escala o unidades diferentes entre las variables originales.   

**Desventajas:**    

- **Interpretación**: La interpretación de los componentes principales puede ser difícil, especialmente si no se tiene un conocimiento profundo del conjunto de datos.    
- **Pérdida de información**: PCA puede eliminar información útil si se descartan componentes con baja variabilidad que podrían ser importantes para la descripción de los datos.   
- **Sensibilidad a los datos atípicos**: PCA puede verse afectado por datos atípicos, resultando en componentes que no reflejan adecuadamente la variabilidad en los datos.    
- **Requisitos computacionales**: PCA puede requerir muchos recursos computacionales, especialmente para conjuntos de datos grandes.  

### Aplicación del PCA {-}  

El PCA es una técnica estadística no supervisada que permite reducir la complejidad de múltiples dimensiones, conservando la información en unas pocas componentes. Dada una matriz de datos con $p \times p$ variables y $n \times n$ observaciones, donde los datos están centrados y escalados con su desviación estándar, se asegura que la nube de datos esté centrada en el origen de las componentes principales, sin verse afectada por las relaciones espaciales entre las variables. 

El PCA busca transformar el espacio vectorial generado por un vector $\mathrm{X=}\left(\mathrm{\ }X_{n1},\ {\ X}_{n2},\ ...\ ,\ X_{np}\right)$ en un nuevo conjunto $\mathrm{Z\ =\ }\left(\mathrm{\ }Z_1,\ {\ Z}_2,\ ...\ ,\ Z_k\right),\ \ k\le p$, que sean combinaciones lineales de los indicadores y que expliquen la mayor parte de la variabilidad. Esto se expresa como:  

$$Z\ =\ XA$$
donde:  
$A\ =\ \left(a_{n1},\ a_{n2},\ ...,\ a_{np}\right)$ representa las posiciones de cada observación en este nuevo sistema de coordenadas de componentes principales, denominadas *loadings*, y se calculan como combinaciones lineales de las variables originales y los pesos $a_{np}$.    

El PCA también permite conocer la proporción de la varianza explicada por cada componente principal, además de la varianza total presente en un conjunto de datos:   

$$\sum_{j=1}^{p}{Var\left(X_j\right)\ =}\ \sum_{j=1}^{p}\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2$$
La varianza explicada por el $k-ésima$ componente principal se define como:  

$$\frac{1}{n}\sum_{1=1}^{n}z_{ik}^2=\frac{1}{n}\sum_{1=1}^{n}\left(\sum_{j=1}^{p}{a_{nj}x_{ij}}\right)^2\ $$ 

Por lo tanto, la proporción de la varianza explicada del $k-ésimo$ componente principal está dada por:    

$$\frac{\sum_{i=1}^{n}\left(\sum_{j=1}^{p}{a_{nj}x_{ij}}\right)^2}{\sum_{j=1}^{p}\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2}$$

En total, hay $min(n\ -\ 1,\ \ p))$ componentes principales, y la suma de la proporción de la varianza explicada es uno.  

### Validación del PCA {-}  

Para validar el PCA, se considera la calidad de la representación de las variables en la construcción del modelo, lo que permite evaluar la estabilidad de las variables en los componentes. También se identifican los valores atípicos y se determina en qué medida influyen en la construcción de los indicadores.   


### Descomposición de componentes principales {-}  

En el análisis de componentes principales, se puede descomponer la matriz de varianzas y covarianzas de dos maneras diferentes:

Descomposición en valores propios (eigendecomposition): Esta descomposición se utiliza cuando la matriz de varianzas y covarianzas es simétrica y definida positiva. En este caso, la matriz se descompone en una matriz de autovectores y una matriz diagonal de valores propios. Los autovectores forman una base ortogonal que describe las direcciones principales de la varianza en los datos, mientras que los valores propios indican la importancia relativa de cada dirección.

Descomposición en valores singulares (singular value decomposition, SVD): Esta descomposición se utiliza cuando la matriz de varianzas y covarianzas no es simétrica o no es definida positiva. En este caso, la matriz se descompone en tres matrices: una matriz de autovectores izquierdos, una matriz de autovectores derechos y una matriz diagonal de valores singulares. Los autovectores izquierdos y derechos forman una base ortogonal que describe las direcciones principales de la varianza en los datos, mientras que los valores singulares indican la importancia relativa de cada dirección.

En ambas descomposiciones, se pueden seleccionar las componentes principales más importantes para reducir la dimensionalidad de los datos y simplificar su análisis.   

<a href="img/SVD.png" data-lightbox="image-1" data-title="SVD">
```{r, echo = FALSE, fig.width=12, fig.height=12, out.width='100%'}
require(knitr)
knitr::include_graphics(paste0(here::here(), "/img/SVD.png"))
```
</a>

<a href="img/SVD_Page_2.png" data-lightbox="image-1" data-title="SVD_2">
```{r, echo = FALSE, fig.width=8, fig.height=8, out.width='100%'}
knitr::include_graphics(paste0(here::here(), "/img/SVD_Page_2.png"))
```
</a>



### Identificación de datos atípicos {-}   

Los datos atípicos (también conocidos como *valores extremos* o *outliers*) pueden tener un impacto significativo en los resultados del método de componentes principales (`PCA`). Si los datos atípicos no se manejan adecuadamente, pueden llevar a resultados sesgados o incorrectos. Por lo tanto, es importante identificarlos y considerar cuidadosamente cómo manejarlos antes de realizar el análisis de PCA.        

Existen algunas estrategias para manejar los datos atípicos dentro del análisis de  PCA:

- **Identificación de los datos atípicos**: Es importante identificar los datos atípicos antes de realizar el análisis de PCA. Esto se puede hacer mediante la visualización de los datos o mediante pruebas estadísticas.      

- **Exclusión de los datos atípicos**: Una estrategia común para manejar los datos atípicos es excluirlos del análisis. Sin embargo, esto debe hacerse con cuidado, ya que la exclusión de los datos atípicos puede resultar en una pérdida de información importante.    

- **Transformación de los datos**: Otra estrategia es transformar los datos antes de realizar el análisis de PCA. Las transformaciones comunes incluyen la transformación logarítmica o la normalización de los datos. Estas transformaciones pueden reducir la influencia de los datos atípicos y mejorar los resultados de PCA.     

- **Uso de métodos robustos**: Existen métodos robustos que pueden manejar los datos atípicos de manera efectiva en PCA, como el método de componentes principales robustos (`RPCA`, por sus siglas en inglés). Estos métodos son menos sensibles a los datos atípicos y pueden proporcionar resultados más precisos.        


### Método de Componentes Principales Robustos `RPCA`  {-}

El método de componentes principales robustos (`RPCA`, por sus siglas en inglés) se utiliza cuando se sospecha que los datos contienen valores atípicos o errores que pueden afectar los resultados del análisis de componentes principales (PCA).    

El RPCA es una versión modificada del método de componentes principales tradicional que es menos sensible a los valores atípicos y a los errores en los datos. En lugar de minimizar la varianza total de los datos como en PCA, RPCA minimiza la varianza total de los datos después de excluir los valores atípicos o errores. Ete tipo de análisis puede ser útil en una variedad de situaciones, como en el análisis de datos biomédicos, análisis financiero, análisis de datos climáticos, entre otros.     
 
En general, se recomienda utilizar RPCA cuando:

- Los datos pueden contener valores atípicos o errores.   

- Los datos tienen una distribución no normal.    

- Las variables tienen diferentes escalas.    

- El tamaño de la muestra es pequeño en comparación con la dimensión de los datos.    

- Se necesita una mayor precisión en los resultados del análisis.    

**Observaciones del método RPCA**

Es importante tener en cuenta que RPCA no siempre es necesario y puede ser computacionalmente más costoso que el método de componentes principales tradicional. Por lo tanto, es importante evaluar cuidadosamente si RPCA es apropiado para los datos que se estan tratando y los objetivos de análisis antes de utilizarlo.      

Aunque el método de componentes principales robustos (RPCA), es útil para manejar valores atípicos y errores en los datos, también presenta algunas desventajas que deben tenerse en cuenta:    

- **Mayor complejidad computacional**: El método RPCA puede ser más costoso computacionalmente que el método de componentes principales tradicional debido a la necesidad de excluir los valores atípicos o errores en los datos.    

- **Pérdida de información**: Al excluir los valores atípicos o errores en los datos, puede perderse información importante, lo que puede afectar la precisión de los resultados del análisis.   

- **Dependencia del modelo**: El método RPCA se basa en un modelo específico de distribución de datos, por lo que si los datos no se ajustan a este modelo, los resultados pueden ser incorrectos o inexactos.    

- **Selección de parámetros**: El método RPCA requiere la selección de parámetros para determinar qué valores se consideran atípicos o errores, lo que puede ser subjetivo y afectar los resultados del análisis.     


## Tercera etapa: Regresión de Componentes Principales (`PCR`) {-}

El modelo de PCR es un método utilizado para reducir la dimensionalidad de los datos y evitar problemas de multicolinealidad en un modelo de regresión lineal múltiple. Este modelo se basa en la descomposición en valores singulares (SVD) de la matriz de datos originales, lo que permite identificar las componentes principales que explican la mayor parte de la varianza en los datos.

A partir de las componentes principales seleccionadas, se construye un modelo de regresión lineal múltiple utilizando estas componentes como variables predictoras. El número de componentes principales seleccionadas se determina de acuerdo con un criterio predefinido, como la cantidad de varianza explicada o un límite para el número de componentes.

El modelo de PCR puede ser una alternativa útil a la selección tradicional de variables predictoras en un modelo de regresión lineal, ya que permite reducir la dimensionalidad de los datos y evitar problemas de multicolinealidad. Sin embargo, es importante tener en cuenta que el modelo de PCR también tiene algunas limitaciones, como la interpretación de los coeficientes de regresión y la posible pérdida de información importante en los datos. Por lo tanto, se recomienda utilizar el modelo de PCR en combinación con otras técnicas de análisis de datos para obtener una comprensión completa del problema.


**Singular value descomposición**  

En el modelo de Regresión de Componentes Principales (PCR, por sus siglas en inglés), se utiliza la descomposición en valores singulares (SVD) para encontrar las componentes principales de los datos y reducir su dimensionalidad.

La idea detrás del PCR es utilizar las componentes principales de los datos (obtenidas a través de la SVD) como variables predictoras en un modelo de regresión lineal múltiple. De esta manera, se puede reducir el número de variables predictoras y evitar problemas de multicolinealidad, lo que puede mejorar la precisión del modelo y hacerlo más interpretable.

En resumen, para implementar el modelo de PCR se utilizan los resultados de la descomposición en valores singulares (SVD) para seleccionar las componentes principales de los datos y construir un modelo de regresión lineal múltiple. Por lo tanto, no se utiliza la descomposición en eigenvectores y eigenvalores.  


## Método de distancias ponderadas al cuadrado $DP_2$ {-}

El método de distancias ponderadas al cuadrado ($DP_2$) de José Bernardo Peña Trapero es una técnica estadística utilizada para medir la similitud entre diferentes objetos o casos. Este método se basa en una medida de distancia que se calcula sumando las diferencias al cuadrado ponderadas entre las características de dos objetos y tomando la raíz cuadrada del resultado. La ponderación de las características permite que las más importantes tengan un mayor impacto en el resultado final mientras que las menos importantes tienen un menor impacto.

El método DP2 de Peña Trapero se utiliza en una variedad de aplicaciones, como la clasificación de imágenes y la evaluación de la similitud entre diferentes textos. Una de las ventajas del método $DP_2$ es que puede manejar datos de diferentes tipos y escalas, lo que lo hace útil en situaciones donde los datos son heterogéneos.

Sin embargo, también hay algunas desventajas en el uso del método DP2, como la selección subjetiva de la función de ponderación de características y el costo computacional en grandes conjuntos de datos. Por lo tanto, se deben evaluar cuidadosamente los datos y los objetivos de análisis antes de aplicar este método.

En resumen, el método DP2 de Peña Trapero es una técnica útil para medir la similitud entre diferentes objetos o casos, pero se debe aplicar con cuidado y evaluar cuidadosamente en función de los datos y objetivos de análisis.

